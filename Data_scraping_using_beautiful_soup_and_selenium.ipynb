{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYSWnpQMdjLaI3rT7bW38s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saiesh-Halshikar/Web-scraping-with-beautiful-soup-and-selenium/blob/main/Data_scraping_using_beautiful_soup_and_selenium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install msedge-selenium-tools\n",
        "!pip install bs4"
      ],
      "metadata": {
        "id": "rBJEl5lcprcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai4pPJW0rch7"
      },
      "source": [
        "from selenium import webdriver\n",
        "import chromedriver_binary\n",
        "from bs4 import BeautifulSoup\n",
        "#for microsoft edge\n",
        "from msedge.selenium_tools import Edge, EdgeOptions\n",
        "import csv\n",
        "\n",
        "#We will be using functions to achieve this\n",
        "\n",
        "def my_url(keyword):\n",
        "    temp = 'https://www.amazon.in/s?k={}&ref=nb_sb_noss_1'\n",
        "    keyword = keyword.replace(' ', '+')\n",
        "    \n",
        "    # Add Term Query To URL\n",
        "    url = temp.format(keyword)\n",
        "    \n",
        "    # Add Page Query Placeholder\n",
        "    url += '&page{}'\n",
        "    \n",
        "    return url\n",
        "\n",
        "def extract_record(obj):\n",
        "    atag = obj.h2.a\n",
        "    description = atag.text.strip()\n",
        "    url = 'https://www.amazon.com' + atag.get('href')\n",
        "    \n",
        "    #it is possible that some items on amazom.com might not be having one of the items we are looking for(e.g. some items might not be having ratings or price), we will be getting error if we dont take care of that. We will therefore add some error handlers\n",
        "    #if there are no price,probably the item is out of stock or not available, then we will ignore the item, but if there are no reviews yet, it's fine, we will still want to extract the item.\n",
        "    try:\n",
        "        parent=obj.find('span','a-price')\n",
        "        price=parent.find('span','a-offscreen').text\n",
        "    except AttributeError: #we are excepting the error if it occurs so that we can move to extract the next item, else the program will stop running and gives error\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        rate=obj.i.text\n",
        "        counts_review = obj.find('span', {'class': 'a-size-base', 'dir': 'auto'}).text\n",
        "    except AttributeError:\n",
        "        #assigning empty string to ratings and \n",
        "        rate = ''\n",
        "        counts_review = ''\n",
        "    \n",
        "    image = obj.find('img', {'class': 's-image'}).get('src') \n",
        "    \n",
        "    #let's create a tuple that will contain all these items and assign it to a result variable\n",
        "    result = (description, price, rate, counts_review, url,image)\n",
        "    return result\n",
        "\n",
        "'''Run Main Program Routine'''\n",
        "def main(keyword):\n",
        "    # Startup The Webdriver\n",
        "    driver = webdriver.Chrome()\n",
        "#     options = EdgeOptions()\n",
        "#     options.use_chromium =True\n",
        "#     driver = Edge(options=options)\n",
        "    \n",
        "    records = []  #an empty records list to contain all of our extracted records\n",
        "    url =my_url(keyword)\n",
        "    \n",
        "    for page in range(1, 50):\n",
        "        driver.get(url.format(page))\n",
        "        soup =BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
        "#         results=soup.find_all('div',{'data-component-type': 's-search-result'}) #same as we did above\n",
        "\n",
        "        \n",
        "#we will like to check if what we have return from the extract_record function is empty or not\n",
        "        for item in results:\n",
        "            record = extract_record(item) \n",
        "            if record: #if the record has something in it append to records list\n",
        "                records.append(record) \n",
        "                \n",
        "#         driver.quit()\n",
        "    \n",
        "#     # Save Results To CSV File\n",
        "        with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Description', 'Price', 'Rating', 'Reviews Count', 'URL','Image link'])\n",
        "            writer.writerows(records)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}